{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814b54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@ Author:         姜楠（小组成员：姜楠、王勃栋、李甜）\n",
    "@ Create Date:    2021-12-01\n",
    "@ Requirements:\n",
    "1, Implmenting the naïve bayes classifier without using sk-learn.\n",
    "2, Implementation need support both discrete and continuous features, and Gaussian, Multinomial, and Bernoulli models required.\n",
    "3, Providing prediction probability is required \n",
    "4, classifier evaluation required.\n",
    "5, Compare your implementation with Sk-learn regressor.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# 定义gaussion model\n",
    "class Gaussion:\n",
    "    def separate_by_classes(self, X, y):\n",
    "        ''' This function separates our dataset in subdatasets by classes '''\n",
    "        self.classes = np.unique(y)\n",
    "        classes_index = {}\n",
    "        subdatasets = {}\n",
    "        cls, counts = np.unique(y, return_counts=True)\n",
    "        self.class_freq = dict(zip(cls, counts))\n",
    "#         print(self.class_freq)\n",
    "        for class_type in self.classes:\n",
    "            classes_index[class_type] = np.argwhere(y==class_type)\n",
    "            subdatasets[class_type] = X[classes_index[class_type], :]\n",
    "            self.class_freq[class_type] = self.class_freq[class_type]/sum(list(self.class_freq.values()))\n",
    "        return subdatasets\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ''' The fitting function '''\n",
    "        separated_X = self.separate_by_classes(X, y)\n",
    "        self.means = {}\n",
    "        self.std = {}\n",
    "        for class_type in self.classes:\n",
    "            # Here we calculate the mean and the standart deviation from datasets\n",
    "            self.means[class_type] = np.mean(separated_X[class_type], axis=0)[0]\n",
    "            self.std[class_type] = np.std(separated_X[class_type], axis=0)[0]\n",
    "            \n",
    "    def calculate_probability(self, x, mean, stdev):\n",
    "        ''' This function calculates the class probability using gaussian distribution '''\n",
    "        exponent = math.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        ''' This function predicts the probability for every class '''\n",
    "        self.class_prob = {cls:math.log(self.class_freq[cls], math.e) for cls in self.classes}\n",
    "        for cls in self.classes:\n",
    "            for i in range(len(self.means)):\n",
    "#                 print(X[i])\n",
    "                self.class_prob[cls]+=math.log(self.calculate_probability(X[i], self.means[cls][i], self.std[cls][i]), math.e)\n",
    "        self.class_prob = {cls: math.e**self.class_prob[cls] for cls in self.class_prob}\n",
    "        return self.class_prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' This funtion predicts the class of a sample '''\n",
    "        pred = []\n",
    "        for x in X:\n",
    "            pred_class = None\n",
    "            max_prob = 0\n",
    "            for cls, prob in self.predict_proba(x).items():\n",
    "                if prob>max_prob:\n",
    "                    max_prob = prob\n",
    "                    pred_class = cls\n",
    "            pred.append(pred_class)\n",
    "        return pred\n",
    "\n",
    "# 定义Multinomial model\n",
    "class Multinomial:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.cat0_count = 0\n",
    "        self.cat1_count = 0\n",
    "        self.total_count = self.cat0_count + self.cat1_count\n",
    "        self.cat_0_prior = 0\n",
    "        self.cat_1_prior = 0\n",
    "        self.cat_0_prior, self.cat_1_prior\n",
    "        self.word_probs = []\n",
    "        self.vocab = []\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Take in a document and return a list of words\n",
    "        \"\"\"\n",
    "        doc = document.lower()\n",
    "#         doc = document\n",
    "        # remove non-alpha characters\n",
    "#         stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "        tokens = \"\"\n",
    "        # iterate through and make each token\n",
    "        for char in doc:\n",
    "#             if char not in stop_chars:\n",
    "            tokens += char\n",
    "\n",
    "        return tokens.split() # now a list of tokens\n",
    "\n",
    "    def count_words(self, X, y):\n",
    "        \"\"\"\n",
    "        X is an array of documents\n",
    "        y is an array of targets, 0 or 1\n",
    "        Output a dictionary of {word: (cat0_count, cat1_count)...}\n",
    "        \"\"\"\n",
    "        counts = {}\n",
    "        # need to figure our this loop, want to iterate over both of them, I see why it was paired before\n",
    "        for document, category in zip(X, y):\n",
    "            for token in self.tokenize(document):\n",
    "              # Initialize a dict entry with 0 counts\n",
    "              if token not in counts:\n",
    "                counts[token] = [0,0]\n",
    "              # Now that it exists, add to the category count for that word\n",
    "              counts[token][category] += 1\n",
    "        return counts\n",
    "\n",
    "    def prior_prob(self, counts):\n",
    "\n",
    "        # Iterate through counts dict and add up each word count by category\n",
    "        cat0_word_count = cat1_word_count = 0\n",
    "        for word, (cat0_count, cat1_count) in counts.items():\n",
    "            cat0_word_count += cat0_count\n",
    "            cat1_word_count += cat1_count\n",
    "\n",
    "        # save attributes to the class\n",
    "        self.cat0_count = cat0_word_count\n",
    "        self.cat1_count = cat1_word_count\n",
    "        self.total_count = self.cat0_count + self.cat1_count\n",
    "\n",
    "        # Get the prior prob by dividing words in each cat by total words\n",
    "        cat_0_prior = cat0_word_count / self.total_count\n",
    "        cat_1_prior = cat1_word_count / self.total_count\n",
    "        return cat_0_prior, cat_1_prior\n",
    "\n",
    "    def word_probabilities(self, counts):\n",
    "        \"\"\"turn the word_counts into a list of triplets\n",
    "        word, p(w | cat0), and p(w | cat1)\"\"\"\n",
    "        # Here we apply the smoothing term, self.k, so that words that aren't in\n",
    "        # the category don't get calculated as 0\n",
    "        self.vocab = [word for word, (cat0, cat1) in counts.items()]\n",
    "        return [(word,\n",
    "        (cat0 + self.k) / (self.cat0_count + 2 * self.k),\n",
    "        (cat1 + self.k) / (self.cat1_count + 2 * self.k))\n",
    "        for word, (cat0, cat1) in counts.items()]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Take all these functions and establish probabilities of input\n",
    "        counts = self.count_words(X, y)\n",
    "        self.cat_0_prior, self.cat_1_prior = self.prior_prob(counts)\n",
    "        self.word_probs = self.word_probabilities(counts)\n",
    "\n",
    "    def predict(self, test_corpus):\n",
    "        # Split the text into tokens,\n",
    "        # For each category: calculate the probability of each word in that cat\n",
    "        # find the product of all of them and the prior prob of that cat\n",
    "        y_pred = []\n",
    "        for document in test_corpus:\n",
    "          # Every document get their own prediction probability\n",
    "            log_prob_cat0 = log_prob_cat1 = 0.0\n",
    "            tokens = self.tokenize(document)\n",
    "            # Iterate through the training vocabulary and add any log probs that match\n",
    "            # if no match don't do anything. We just need a score for each category/doc\n",
    "            for word, prob_cat0, prob_cat1 in self.word_probs:\n",
    "                if word in tokens:\n",
    "                  # Because of 'overflow' best to add the log probs together and exp\n",
    "                    log_prob_cat0 += np.log(prob_cat0)\n",
    "                    log_prob_cat1 += np.log(prob_cat1)\n",
    "            # get each of the category predictions including the prior\n",
    "            cat_0_pred = self.cat_0_prior * np.exp(log_prob_cat0)\n",
    "            cat_1_pred = self.cat_1_prior * np.exp(log_prob_cat1)\n",
    "            if cat_0_pred >= cat_1_pred:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# 定义Bernoulli model\n",
    "class Bernoulli:\n",
    "    def __init__(self, smooth=1):\n",
    "        self._smooth = smooth\n",
    "        self._feat_prob = {}\n",
    "        self._class_prob = {}\n",
    "        self._classDict = {}\n",
    "        self._featureMap = {}\n",
    "        self._Ncls = 0\n",
    "        self._Nfeat = 0\n",
    "\n",
    "    def computeClassProbability(self, totalRecords):\n",
    "        for x in self._classDict.keys():\n",
    "            self._class_prob[x] = (self._classDict[x] + self._smooth) / float(totalRecords + (2**self._smooth)) # 2 because bernoullli (2 options)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        classDict = self.makeClasses(y)\n",
    "        numFeatures = X.shape[1]\n",
    "        classMap = {k: {\"count\": 0, \"probability\": 0} for k in classDict.keys()}\n",
    "        featureMap = {k: classMap for k in range(0,numFeatures)}\n",
    "        #featureMap contains a dictionary for every feature\n",
    "        #which contains a dictionary for every class, and stores zero_count given class\n",
    "\n",
    "        #go through the array and add all the counts for 0 given Class C\n",
    "        for row in range(0, X.shape[0]):\n",
    "            for column in range(0, X.shape[1]):\n",
    "                val = 0 | X[row][column]\n",
    "                if(val == 0):\n",
    "                    correspondingClassValue = y[row]\n",
    "                    featureMap[column][correspondingClassValue][\"count\"] += 1\n",
    "                if(row == X.shape[0] - 1):\n",
    "                    # calculate probablity and account for alpha\n",
    "                    featureMap[column][correspondingClassValue][\"probability\"] = (featureMap[column][correspondingClassValue][\"count\"] + self._smooth) / float(classDict[correspondingClassValue] + (2**self._smooth))\n",
    "\n",
    "        self._featureMap = featureMap\n",
    "        self._Ncls = len(classDict.keys())\n",
    "        self._Nfeat = numFeatures\n",
    "        self._classDict = classDict\n",
    "        self._class_prob = self.computeClassProbability(X.shape[0])\n",
    "\n",
    "    def getPointProbability(self, featureName, columnName, className, value):\n",
    "\n",
    "        retrieved = self._featureMap[fname][str(className)][\"probability\"]\n",
    "        if(val == 1):\n",
    "            retrieved = 1 - retrieved\n",
    "        return retrieved\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        results = np.zeros([X.shape[0],1])\n",
    "        for row in range(0, X.shape[0]):\n",
    "            maxVal = 0\n",
    "            finalChoice = -1\n",
    "            for className in self._classDict.keys():\n",
    "                partialTotal = self._class_prob[str(className)]\n",
    "                for column in range(0, X.shape[1]):\n",
    "                    partialTotal *= self.getPointProbability(row,column,className,X[row][column])\n",
    "                #set maxVal/result\n",
    "                if(maxVal<partialTotal):\n",
    "                    maxVal = partialTotal\n",
    "                    finalChoice = className\n",
    "            results[row] = finalChoice\n",
    "\n",
    "        return results\n",
    "\n",
    "    def makeClasses(self, y):\n",
    "        classes = {}\n",
    "        for i in range(0,len(y)):\n",
    "            val = y[i]\n",
    "            if(val in classes):\n",
    "                classes[val] += 1\n",
    "            else:\n",
    "                classes[val] = 1\n",
    "        return classes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a11d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集准备\n",
    "from sklearn import datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# gaussian 使用sklearn iris数据集\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6819a050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB predict: [2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# 使用sklearn GaussianNB模型预测\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB ,MultinomialNB\n",
    "\n",
    "#GaussianNB\n",
    "skg=GaussianNB()\n",
    "skg.fit(X_train,y_train)\n",
    "skg_pre = skg.predict(X_test)\n",
    "print(\"GaussianNB predict:\",skg_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195670a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussion predict: [1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0, 0, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# 使用编写的 GaussianNB模型预测\n",
    "g=Gaussion()\n",
    "g.fit(X_train,y_train)\n",
    "g_pre = g.predict(X_test)\n",
    "print(\"Gaussion predict:\",g_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400bc6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      text                                                               \n",
      "     count unique                                                top freq\n",
      "spam                                                                     \n",
      "0       82     82                        Anything lor... U decide...    1\n",
      "1       17     17  As a valued customer, I am pleased to advise y...    1\n",
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "# multiminol 使用 邮件数据集\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "spam = pd.read_csv(\"spam.csv\")\n",
    "dummies = pd.get_dummies(spam.label)\n",
    "spam = pd.concat([spam,dummies],axis=\"columns\")\n",
    "spam = spam.drop([\"label\",\"ham\"],axis=\"columns\")\n",
    "print(spam.groupby(\"spam\").describe())\n",
    "\n",
    "# print(spam)\n",
    "m_X_train, m_X_test, m_y_train, m_y_test = train_test_split(spam[\"text\"], spam[\"spam\"], test_size=0.7, random_state=0)\n",
    "# v=CountVectorizer(analyzer='word',ngram_range=(2,2))\n",
    "v=CountVectorizer(stop_words='english')\n",
    "\n",
    "m_X_train_T=v.fit_transform(m_X_train.values)\n",
    "m_X_test_T=v.transform(m_X_test.values)\n",
    "\n",
    "print(m_X_train_T.toarray()[:1])\n",
    "# v.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d006d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB predict: [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "\n",
    "skm = MultinomialNB()\n",
    "skm.fit(m_X_train_T, m_y_train)\n",
    "skm_pre = skm.predict(m_X_test_T);\n",
    "print(\"MultinomialNB predict:\",skm_pre)\n",
    "# skm.score(m_X_train_T,m_y_test)\n",
    "# skm_pre.reshape(1,3)\n",
    "# m_X_test_T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f767636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial predict: [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#使用自编Multinomial\n",
    "m=Multinomial()\n",
    "m.fit(m_X_train, m_y_train)\n",
    "m_pre = m.predict(m_X_test)\n",
    "print(\"Multinomial predict:\",m_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5213c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bernoulli数据集，使用multiminal转换\n",
    "v=CountVectorizer(stop_words='english', binary=True)\n",
    "\n",
    "b_X_train_T=v.fit_transform(m_X_train.values).toarray()\n",
    "b_X_test_T=v.transform(m_X_test.values).toarray()\n",
    "b_y_train=m_y_train\n",
    "# print(b_X_train_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1b88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB predict: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# skleran 实现，此处使用multiminal数据集\n",
    "skb = BernoulliNB()\n",
    "skb.fit(m_X_train_T, b_y_train)\n",
    "print(\"BernoulliNB predict:\",skb.predict(b_X_test_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea0bccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3735ecfb7ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(b_X_train_T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mb_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_X_test_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# b_pre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print 'alpha=%i accuracy = %f' %(alpha, np.mean((y_test-y_pred)==0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-70eed80d4449>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mfinalChoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclassName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mpartialTotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0mpartialTotal\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPointProbability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#自编bernuoulli\n",
    "alpha = 1\n",
    "b = Bernoulli(alpha)\n",
    "b.fit(b_X_train_T, np.array(b_y_train))\n",
    "\n",
    "# print(b_X_train_T)\n",
    "b_pre = b.predict(b_X_test_T)\n",
    "# b_pre\n",
    "# print 'alpha=%i accuracy = %f' %(alpha, np.mean((y_test-y_pred)==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2496b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6be65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6e075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
